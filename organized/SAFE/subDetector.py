from malwareDetector.detector import detector
from typing import Any
import numpy as np
import os
from utils.function_normalizer import FunctionNormalizer
from utils.instructions_converter import InstructionsConverter
from utils.capstone_disassembler import disassemble
from utils.radare_analyzer import BinaryAnalyzer
from safetorch.safe_network import SAFE
from safetorch.parameters import Config
import r2pipe
import networkx as nx
from gnn.model import MalwareExpert
from gnn.utils import load_data, train, GNNexplain, edge_pruning, node_pruning
import torch_geometric
import torch
from time import time

class subDetector(detector):
    def __init__(self) -> None:
        super().__init__()
        self.train_data = []
        self.test_data = []
        self.predict_data = []
        self.unlearn_data = []
        self.shard_index = 0
        self.slice_index = 0
        self.model_params = None
        self.train_loss = []
        self.val_loss = []

    def extractFeature(self, logger, purpose:str='train') -> Any:
        print('Extracting and Vectorizing the features...')
        logger.info('Extracting and Vectorizing the features...')
        if purpose == 'train':
            binary_path = subDetector().config.folder.dataset
            current_data = self.train_data
        elif purpose == 'test':
            binary_path = subDetector().config.folder.dataset
            current_data = self.test_data
        elif purpose == 'predict':
            binary_path = subDetector().config.folder.predict
            current_data = self.predict_data

        os.makedirs(subDetector().config.folder.embedding, exist_ok=True)

        for file in current_data:
            # if file is already done, skip
            if os.path.exists(subDetector().config.folder.embedding+"{}.dot".format(file)):
                print(f'{file} is already done')
                continue
            else:
                malware = os.path.join(binary_path, file)
                print(malware)

                # create cfg
                r2 = r2pipe.open(malware)
                r2.cmd("aaa")
                r2.cmd("agCd > output.dot")
                # netwokx graph from dot string
                G = nx.nx_pydot.read_dot("output.dot")
                r2.quit()

                # initialize SAFE
                config = Config()
                safe = SAFE(config)

                # load instruction converter and normalizer
                I2V_FILENAME = subDetector().config.path.word2id
                converter = InstructionsConverter(I2V_FILENAME)
                normalizer = FunctionNormalizer(max_instruction=150)

                # load SAFE weights
                SAFE_torch_model_path = subDetector().config.path.SAFEtorch
                state_dict = torch.load(SAFE_torch_model_path)
                safe.load_state_dict(state_dict)
                safe = safe.eval()

                # analyze the binary
                binary = BinaryAnalyzer(malware)
                offsets = binary.get_functions()

                # generate each function embedding
                for offset in offsets:
                    asm = binary.get_hexasm(offset)
                    instructions = disassemble(asm, binary.arch, binary.bits)
                    converted_instructions = converter.convert_to_ids(instructions)
                    instructions, length = normalizer.normalize_functions(
                        [converted_instructions])
                    tensor = torch.LongTensor(instructions[0])
                    function_embedding = safe(tensor, length)

                    #change tensor to numpy
                    function_embedding = function_embedding.detach().numpy()

                    offset_temp = str(hex(offset))[2:]
                    for i in range(8 - len(offset_temp)):
                        offset_temp = '0' + offset_temp
                    offset_temp = '0x' + offset_temp

                    #find the node in the graph and add the embedding to the node
                    if G.has_node(offset_temp):
                        G.nodes[offset_temp]['embedding'] = function_embedding

                #output the graph
                nx.nx_pydot.write_dot(G, subDetector().config.folder.embedding+"{}.dot".format(file))
        

    def vectorize(self, logger, purpose:str='train') -> np.array:
        return None

    def model(self, logger, training:bool=True) -> Any:
        #print(f'current model: model_{self.shard_index}_{self.slice_index}.pt')
        if training:
            # a normal training process, no sisa
            print(f'Training model_{self.shard_index}_{self.slice_index}.pt...')
            logger.info(f'Training model_{self.shard_index}_{self.slice_index}.pt...')
            start = time()
            # load data
            feature_matrix, edge_index, label, name = load_data(embedding_files=self.train_data)
            x = [torch.tensor(feature_matrix[i], dtype=torch.float) for i in range(len(feature_matrix))]
            edge_index = [torch.tensor(edge_index[i], dtype=torch.long) for i in range(len(edge_index))]
            y = [torch.tensor(label[i], dtype=torch.long) for i in range(len(label))]
            name = [name[i] for i in range(len(name))]
            all_data = [torch_geometric.data.Data(x=x[i], edge_index=edge_index[i], y=y[i], name=name[i]) for i in range(len(x))]

            # create model
            if self.model_params == None:
                model = MalwareExpert(100, subDetector().config.model.hidden_dim)
            else:
                model = self.model_params

            # criterion and optimizer
            criterion = torch.nn.CrossEntropyLoss()
            optimizers = torch.optim.Adam(model.parameters(), lr=subDetector().config.model.learning_rate)

            #train 0.9
            train_data = all_data[:int(len(all_data)*0.9)]
            train_data = torch_geometric.data.Batch.from_data_list(train_data)
            train_data = torch_geometric.loader.DataLoader(train_data, batch_size=subDetector().config.model.batch_size, shuffle=True)
            #val 0.1
            val_data = all_data[int(len(all_data)*0.9):]
            val_data = torch_geometric.data.Batch.from_data_list(val_data)
            val_data = torch_geometric.loader.DataLoader(val_data, batch_size=subDetector().config.model.batch_size, shuffle=True)
            for epoch in range(subDetector().config.model.epoch):
                train_loss = train(model, train_data, criterion, optimizers)
                print(all_data)
                print('time: ', time()-start)
                print('epoch: ', epoch, ' loss: ', train_loss)
                self.train_loss.append(train_loss)
                # validation every epoch
                val_loss = 0
                model.eval()
                with torch.no_grad():
                    for data in val_data:
                        output = model(data.x, data.edge_index, data)
                        pred = output.argmax(dim=1)
                        correct = pred.eq(data.y).sum().item()
                        loss = criterion(output, data.y)
                        val_loss += loss.item()
                    print('val_accuracy: ', correct/len(data.y), 'val_loss: ', val_loss)
                    self.val_loss.append(val_loss)
            
            #save the model
            self.model_params = model

        else:
            # load model for later use
            print(f'Loading model_{self.shard_index}_{self.slice_index}.pt...')
            logger.info(f'Loading model_{self.shard_index}_{self.slice_index}.pt...')
            self.model_params = MalwareExpert(100, subDetector().config.model.hidden_dim)
            self.model_params.load_state_dict(torch.load(subDetector().config.folder.model + f'slice_models/model_{self.shard_index}_{self.slice_index}.pt'))
            return self.model_params

    def predict(self, logger, purpose:str='test') -> np.array:
        # normal prediction process, no majority vote
        print(f'Predicting with model_{self.shard_index}_{self.slice_index}.pt...')
        logger.info(f'Predicting with model_{self.shard_index}_{self.slice_index}.pt...')
        if purpose == 'test':
            current_data = self.test_data
        elif purpose == 'predict':
            current_data = self.predict_data

        # load data
        feature_matrix, edge_index, name = load_data(embedding_files=current_data, predict=True)
        x = [torch.tensor(feature_matrix[i], dtype=torch.float) for i in range(len(feature_matrix))]
        edge_index = [torch.tensor(edge_index[i], dtype=torch.long) for i in range(len(edge_index))]
        name = name
        all_data = [torch_geometric.data.Data(x=x[i], edge_index=edge_index[i], name=name[i]) for i in range(len(x))]

        # predict the dataset
        predict = [] #list of predicted labels
        for data in all_data:
            model = self.model_params
            model.eval()
            with torch.no_grad():
                output = model(data.x, data.edge_index, data)
                pred = output.argmax(dim=1)
                predict.append(pred.item())
        
        print('Predicted labels:', predict)
        return predict

    def explain(self, logger) -> Any:
        print(f'Explaining with model_{self.shard_index}_{self.slice_index}.pt...')
        logger.info(f'Explaining with model_{self.shard_index}_{self.slice_index}.pt...')
        current_data = self.predict_data #(or explain_data)

        # load data
        feature_matrix, edge_index, name = load_data(embedding_files=current_data, predict=True)
        x = [torch.tensor(feature_matrix[i], dtype=torch.float) for i in range(len(feature_matrix))]
        edge_index = [torch.tensor(edge_index[i], dtype=torch.long) for i in range(len(edge_index))]
        name = name
        all_data = [torch_geometric.data.Data(x=x[i], edge_index=edge_index[i], name=name[i]) for i in range(len(x))]
        
        #explain the model
        model = self.model_params
        GNNexplain(model, self.shard_index, self.slice_index, all_data)
        edge_pruning(model, self.shard_index, self.slice_index, all_data)
        node_pruning(model, self.shard_index, self.slice_index, all_data)

    