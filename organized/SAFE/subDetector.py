from malwareDetector.detector import detector
from typing import Any
import numpy as np
import os
import subprocess
from utils.function_normalizer import FunctionNormalizer
from utils.instructions_converter import InstructionsConverter
from utils.capstone_disassembler import disassemble
from utils.radare_analyzer import BinaryAnalyzer
from safetorch.safe_network import SAFE
from safetorch.parameters import Config
import r2pipe
import networkx as nx
from gnn.model import MalwareExpert
from gnn.utils import load_data, train
import torch_geometric
import torch
from time import time

class subDetector(detector):
    def __init__(self) -> None:
        super().__init__()
        self.train_data = []
        #test_data = []
        #self.predict_data = []
        #self.unlearn_data = []
        self.shard_index = 0
        self.slice_index = 0
        self.model_params = None

    def extractFeature(self, purpose:str='train') -> Any:
        print('Extracting and Vectorizing the features...')
        # run 'python test.py' to extract features
        #subprocess.run(['python', 'test.py'])
        #return None
        binary_path = subDetector().config.folder.dataset

        os.makedirs(subDetector().config.folder.embedding, exist_ok=True)

        for file in self.train_data:
            malware = os.path.join(binary_path, file)
            print(malware)

            # create cfg
            r2 = r2pipe.open(malware)
            r2.cmd("aaa")
            r2.cmd("agCd > output.dot")
            # netwokx graph from dot string
            G = nx.nx_pydot.read_dot("output.dot")
            r2.quit()

            # initialize SAFE
            config = Config()
            safe = SAFE(config)

            # load instruction converter and normalizer
            I2V_FILENAME = "SAFE_model/word2id.json"
            converter = InstructionsConverter(I2V_FILENAME)
            normalizer = FunctionNormalizer(max_instruction=150)

            # load SAFE weights
            SAFE_torch_model_path = "SAFE_model/SAFEtorch.pt"
            state_dict = torch.load(SAFE_torch_model_path)
            safe.load_state_dict(state_dict)
            safe = safe.eval()

            # analyze the binary
            binary = BinaryAnalyzer(malware)
            offsets = binary.get_functions()

            # generate each function embedding
            for offset in offsets:
                asm = binary.get_hexasm(offset)
                instructions = disassemble(asm, binary.arch, binary.bits)
                converted_instructions = converter.convert_to_ids(instructions)
                instructions, length = normalizer.normalize_functions(
                    [converted_instructions])
                tensor = torch.LongTensor(instructions[0])
                function_embedding = safe(tensor, length)

                #change tensor to numpy
                function_embedding = function_embedding.detach().numpy()
                #print(hex(offset), function_embedding)

                offset_temp = str(hex(offset))[2:]
                for i in range(8 - len(offset_temp)):
                    offset_temp = '0' + offset_temp
                offset_temp = '0x' + offset_temp

                #find the node in the graph and add the embedding to the node
                if G.has_node(offset_temp):
                    G.nodes[offset_temp]['embedding'] = function_embedding

            #output the graph
            nx.nx_pydot.write_dot(G, subDetector().config.folder.embedding+"{}.dot".format(file))
        

    def vectorize(self, purpose:str='train') -> np.array:
        return None

    def model(self, training:bool=True) -> Any:
        if training:
            # a normal training process, no sisa
            print('Training...')
            start = time()
            #subprocess.run(['python', 'gnn/train.py'])
            # load data
            feature_matrix, edge_index, label, name = load_data(embedding_files=self.train_data)
            x = [torch.tensor(feature_matrix[i], dtype=torch.float) for i in range(len(feature_matrix))]
            edge_index = [torch.tensor(edge_index[i], dtype=torch.long) for i in range(len(edge_index))]
            y = [torch.tensor(label[i], dtype=torch.long) for i in range(len(label))]
            name = [name[i] for i in range(len(name))]
            all_data = [torch_geometric.data.Data(x=x[i], edge_index=edge_index[i], y=y[i], name=name[i]) for i in range(len(x))]

            # create model
            # if model is first slice(model_params=None), create a new model
            # else load the previous slice model(use model_params)
            if self.model_params is None:
                model = MalwareExpert(
                    input_dim=100,
                    hidden_dim=subDetector().config.model.hidden_dim
                )
            else:
                model = self.model_params

            # criterion and optimizer
            criterion = torch.nn.CrossEntropyLoss()
            optimizers = torch.optim.Adam(model.parameters(), lr=subDetector().config.model.learning_rate)

            #train 0.9
            train_data = all_data[:int(len(all_data)*0.9)]
            train_data = torch_geometric.data.Batch.from_data_list(train_data)
            train_data = torch_geometric.loader.DataLoader(train_data, batch_size=subDetector().config.model.batch_size, shuffle=True)
            for epoch in range(subDetector().config.model.epoch):
                train_loss = train(model, train_data, criterion, optimizers, epoch)
                print(all_data)
                print('time: ', time()-start)

            #val 0.1
            val_data = all_data[int(len(all_data)*0.9):]
            val_data = torch_geometric.data.Batch.from_data_list(val_data)
            val_data = torch_geometric.loader.DataLoader(val_data, batch_size=subDetector().config.model.batch_size, shuffle=True)
            for data in val_data:
                model.eval()
                with torch.no_grad():
                    output = model(data.x, data.edge_index, data)
                    pred = output.argmax(dim=1)
                    correct = pred.eq(data.y).sum().item()
                    val_loss = criterion(output, data.y)
                    print('val_accuracy: ', correct/len(data.y), 'val_loss: ', val_loss.item())
            
            #save the model
            self.model_params = model

        else:
            #print('Load model and train...')
            #subprocess.run(['python', 'gnn/load.py'])
            # load model for later to predict
            print('Load model...')
            return self.model_params

    def predict(self) -> np.array:
        # normal prediction process, no majority vote
        print('Predicting the dataset...')
        #subprocess.run(['python', 'gnn/predict.py'])

    def explain(self) -> Any:
        print('Explaining the model...')
        #subprocess.run(['python', 'gnn/explain.py'])

    """def machine_unlearning(self) -> Any:
        print('Machine unlearning...')
        subprocess.run(['python', 'gnn/unlearn.py'])"""

"""if __name__ == '__main__':
    myDetector = subDetector()
    #myDetector.extractFeature()
    #print('Training...')
    #os.system('python scripts/train.py')
    #subprocess.call("python scripts/train.py", shell=True)
    myDetector.vectorize()
    #myDetector.model(training=True)
    #myDetector.model(training=False)
    #myDetector.predict()
    #myDetector.explain()
    #myDetector.machine_unlearning()"""
    