
from subDetector import subDetector
import os
import csv
import torch
import pandas as pd
import json
import logging

class SISA():
    def __init__(self)->None:
        self.shard_count = subDetector().config.model.shard_count
        self.slice_count = subDetector().config.model.slice_count
        #initialize n number of subDetectors
        # create a 2D array of subDetectors, each shard has slice_count subDetectors
        self.myDetector = [[subDetector() for j in range(self.slice_count)] for i in range(self.shard_count)]
        for i in range(self.shard_count):
            for j in range(self.slice_count):
                self.myDetector[i][j].shard_index = i
                self.myDetector[i][j].slice_index = j
        pass           

    def preprocess(self, logger, purpose:str='train'):
        logger.info('Preprocessing data...')
        #preprocess the data
        for i in range(self.shard_count):
            for j in range(self.slice_count):
                self.myDetector[i][j].extractFeature(logger, purpose=purpose)
                self.myDetector[i][j].vectorize(logger, purpose=purpose)
        pass

    def divide_data(self, logger):
        print('Dividing data...')
        logger.info('Dividing data...')

        #split the dataset into training 0.8 and testing 0.2
        dataset_dir = subDetector().config.folder.dataset
        dataset = os.listdir(dataset_dir)
        train_data = dataset[:int(len(dataset)*0.8)]
        test_data = dataset[int(len(dataset)*0.8):]
        shard_data = []
        slice_data = []

        #shard the train data
        shard_size = len(train_data) // self.shard_count
        for i in range(self.shard_count):
            if i == self.shard_count - 1:
                shard_data.append(train_data[i*shard_size:])
            else:
                shard_data.append(train_data[i*shard_size:(i+1)*shard_size])

        #slice the shard data
        slice_size = shard_size // self.slice_count
        for i in range(self.shard_count):
            for j in range(self.slice_count):
                if j == self.slice_count - 1:
                    slice_data.append(shard_data[i][j*slice_size:])
                else:
                    slice_data.append(shard_data[i][j*slice_size:(j+1)*slice_size])

        #write the record
        with open(subDetector().config.path.record, 'w') as f:
            records = []
            data = {}
            for i in range(self.shard_count):
                for j in range(self.slice_count):
                    for k in range(len(slice_data[i*self.shard_count+j])):
                        data['purpose'] = 'train'
                        data['name'] = slice_data[i*self.shard_count+j][k]
                        data['shard_index'] = i
                        data['slice_index'] = j
                        data['index_in_slice'] = k
                        data['unlearned'] = 'False'
                        records.append(data)
                        data = {}
                    self.myDetector[i][j].train_data = slice_data[i*self.shard_count+j]
                self.myDetector[i][self.slice_count-1].test_data = test_data
            
            for i in range(len(test_data)):
                data['purpose'] = 'test'
                data['name'] = test_data[i]
                data['shard_index'] = 'None'
                data['slice_index'] = 'None'
                data['index_in_slice'] = 'None'
                data['unlearned'] = 'None'
                records.append(data)
                data = {}

            json.dump(records, f, indent=4)

        pass
        
    def sisa_training(self, logger)->None:
        #model training
        print('sisa training...')
        logger.info('SISA training...')

        # create folders for models
        os.makedirs(subDetector().config.folder.model, exist_ok=True)
        os.makedirs(subDetector().config.folder.model + 'slice_models', exist_ok=True)
        os.makedirs(subDetector().config.folder.model + 'shard_models', exist_ok=True)

        # do sisa train here, each slice one subDetector, load and train for a shard
        for i in range(self.shard_count):
            for j in range(self.slice_count):
                if j == 0:
                    self.myDetector[i][j].model(logger, training=True)
                else:
                    # train data = previous train data + current train data
                    self.myDetector[i][j].train_data += self.myDetector[i][j-1].train_data
                    
                    # load the previous slice model, and train the current slice
                    print('need to load model'+f'_{i}_{j-1}')
                    self.myDetector[i][j].model_params = self.myDetector[i][j-1].model(logger, training=False)
                    self.myDetector[i][j].model(logger, training=True)

                # save the slice model
                torch.save(self.myDetector[i][j].model_params.state_dict(), subDetector().config.folder.model + f'slice_models/model_{i}_{j}.pt')
                # save the shard model
                if j == self.slice_count - 1:
                    torch.save(self.myDetector[i][j].model_params.state_dict(), subDetector().config.folder.model + f'shard_models/model_{i}.pt')

        # after all train finished, do majority vote(testing)
        print('Testing...')
        logger.info('Testing...')
        self.majority_vote(logger, purpose='test')
            
    def majority_vote(self, logger, purpose:str='test'):
        #majority vote
        logger.info('Majority vote...')

        last_slice = self.slice_count - 1
        if purpose == 'predict':
            # get the predict data
            predict_data = os.listdir(subDetector().config.folder.predict)
            for i in range(self.shard_count):
                self.myDetector[i][last_slice].predict_data = predict_data

        # load all shard models and do majority vote
        record = []
        for i in range(self.shard_count):
            self.myDetector[i][last_slice].model(logger, training=False) # load the shard model
            self.myDetector[i][last_slice].extractFeature(logger, purpose=purpose)
            self.myDetector[i][last_slice].vectorize(logger, purpose=purpose)
            record.append(self.myDetector[i][last_slice].predict(logger, purpose=purpose))

        # do majority vote here
        final_result = []
        for i in range(len(record[0])):
            votes = [0, 0]
            for j in range(self.shard_count):
                votes[record[j][i]] += 1
            if votes[0] > votes[1]:
                final_result.append(0)
            else:
                final_result.append(1)

        if purpose == 'test':
            # get labels of the test data
            data = self.myDetector[0][last_slice].test_data
            label_file = pd.read_csv(subDetector().config.path.label)
            unmap_label = label_file['label']
            filenames = label_file['filename']
                
            label = []
            maps = {'malware': 1, 'benignware': 0}
            unmap_label = unmap_label.map(maps)

            #find filname's index and map the label
            for file in data:
                index = filenames[filenames == file].index[0] 
                label.append(unmap_label[index])

            print('final predict: ', final_result)
            print('label: ', label)
            # get the confusion matrix
            # malware is 1, benignware is 0
            tp = 0
            tn = 0
            fp = 0
            fn = 0
            for i in range(len(label)):
                if label[i] == 1 and final_result[i] == 1:
                    tn += 1
                elif label[i] == 0 and final_result[i] == 0:
                    tp += 1
                elif label[i] == 0 and final_result[i] == 1:
                    fn += 1
                elif label[i] == 1 and final_result[i] == 0:
                    fp += 1

            #calculate the accuracy, precision, recall, f1_score
            accuracy = (tp+tn)/(tp+tn+fp+fn)
            if tp+fp == 0:
                precision = 0
            else:
                precision = tp/(tp+fp)
            if tp+fn == 0:
                recall = 0
            else:
                recall = tp/(tp+fn)
            if precision+recall == 0:
                f1_score = 0
            else:
                f1_score = 2*precision*recall/(precision+recall)
            
            #load previous results if exists
            if os.path.exists(subDetector().config.path.score):
                with open(subDetector().config.path.score, 'r') as f:
                    previous_results = json.load(f)
                    
            #output train loss, val loss, scores
            with open(subDetector().config.path.score, 'w') as f:
                results = []
                data = {}
                #the scores
                data['final_result'] = {
                    'TP': tp,
                    'TN': tn,
                    'FP': fp,
                    'FN': fn,
                    'accuracy': accuracy,
                    'precision': precision,
                    'recall': recall,
                    'f1_score': f1_score
                }
                results.append(data)
                data = {}
                #the loss of each model epoch
                for i in range(self.shard_count):
                    for j in range(self.slice_count):
                        data['model_name'] = f'model_{i}_{j}'
                        data['train_loss'] = []
                        data['val_loss'] = []
                        for epoch in range(subDetector().config.model.epoch):
                            if len(self.myDetector[i][j].train_loss) <= 0:
                                train_loss = previous_results[i*self.slice_count+j+1]['train_loss'][epoch]['loss']
                                val_loss = previous_results[i*self.slice_count+j+1]['val_loss'][epoch]['loss']
                            else:
                                train_loss = self.myDetector[i][j].train_loss[epoch]
                                val_loss = self.myDetector[i][j].val_loss[epoch]

                            data['train_loss'].append({
                                'epoch': epoch,
                                'loss': train_loss
                            })
                            data['val_loss'].append({
                                'epoch': epoch,
                                'loss': val_loss
                            })
                        results.append(data)
                        data = {}
                json.dump(results, f, indent=4)                    

        else: #output result.json
            print('final result: ', final_result)
            with open(subDetector().config.path.result, 'w') as f:
                results = []
                data = {}
                for i in range(len(final_result)):
                    data['name'] = predict_data[i]
                    data['result'] = final_result[i]
                    results.append(data)
                    data = {}
                json.dump(results, f, indent=4)

        pass
            
    def machine_unlearning(self, logger):
        #machine unlearning
        logger.info('Machine unlearning...')
        unlearn_data = os.listdir(subDetector().config.folder.unlearn)

        #update the record
        print('update record...')
        with open(subDetector().config.path.record, 'r') as f:
            records = json.load(f)
            for record in records:
                if record['purpose'] == 'train' and record['name'] in unlearn_data:
                    record['unlearned'] = 'True'

        with open(subDetector().config.path.record, 'w') as f:
            json.dump(records, f, indent=4)

        #unlearn the data
        for i in range(self.shard_count):
            print('unlearning shard: ', i)
            unlearn_data = []
            min_slice = self.slice_count - 1

            for record in records:
                if record['purpose'] == 'train':
                    if int(record['shard_index']) == i and record['unlearned'] == 'True':
                        unlearn_data.append(record['name'])
                        if int(record['slice_index']) < min_slice:
                            min_slice = int(record['slice_index'])
                    elif int(record['shard_index']) > i:
                        break

            print('min_slice: ', min_slice)
            print('unlearn_data: ', unlearn_data)
            
            #load the slice data to their model train data
            if len(unlearn_data) > 0:
                for j in range(min_slice, self.slice_count):
                    for record in records:
                        if record['purpose'] == 'train':
                            #if is min_slice, need to load the data before min_slice
                            if j == min_slice and int(record['shard_index']) == i and int(record['slice_index']) <= j and record['unlearned'] == 'False':
                                self.myDetector[i][j].train_data.append(record['name'])
                            elif int(record['shard_index']) == i and int(record['slice_index']) == j and record['unlearned'] == 'False':
                                self.myDetector[i][j].train_data.append(record['name'])
                            elif int(record['shard_index']) > i:
                                break
                
                    if j == 0:
                        print('train a new model')
                        self.myDetector[i][0].model(logger, training=True)
                    else:
                        self.myDetector[i][j].train_data += self.myDetector[i][j-1].train_data
                        print('need to load model'+f'_{i}_{j-1}')
                        self.myDetector[i][j].model_params = self.myDetector[i][j-1].model(logger, training=False)
                        self.myDetector[i][j].model(logger, training=True)

                    #save the slice model
                    torch.save(self.myDetector[i][j].model_params.state_dict(), subDetector().config.folder.model + f'slice_models/model_{i}_{j}.pt')
                    #save the shard model
                    if j == self.slice_count - 1:
                        torch.save(self.myDetector[i][j].model_params.state_dict(), subDetector().config.folder.model + f'shard_models/model_{i}.pt')

        # after all train finished, do majority vote(testing)
        # get test data
        with open(subDetector().config.path.record, 'r') as f:
            records = json.load(f)
            test_data = []
            for record in records:
                if record['purpose'] == 'test':
                    test_data.append(record['name'])
        for i in range(self.shard_count):
            self.myDetector[i][self.slice_count-1].test_data = test_data
        print('Testing...')
        self.majority_vote(logger, purpose='test')
        pass

    def explain(self, logger):
        #explain the model
        logger.info('Explain the model...')
        last_slice = self.slice_count - 1
        for i in range(self.shard_count):
                self.myDetector[i][last_slice].model(logger, training=False)
                self.myDetector[i][last_slice].explain(logger)
        pass
