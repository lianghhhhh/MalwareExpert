from matplotlib import pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
import torch_geometric
import torch_geometric.data
from gnn.model import MalwareExpert
import time
import os
import networkx as nx
import pandas as pd
from torch_geometric.explain import Explainer, GNNExplainer, ModelConfig
import csv

#import subDetector from parent directory
import sys
parend_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), os.path.pardir))
sys.path.append(parend_dir)
#from subDetector import subDetector

#load data
def load_data(embedding_files, predict = False):
    from subDetector import subDetector
    print('loading data...')

    if predict:
        embedding_dir = subDetector().config.folder.predict
    else:
        embedding_dir = subDetector().config.folder.vectorize

    """for i in range(len(data)):
        data[i] = data[i] + '.dot'
    embedding_files = data #the data for unlearning"""

    feature_matrix = []
    edge_index = []
    name = []

    for file in embedding_files:
        name.append(file)
        file = file + '.dot'
        G = nx.nx_pydot.read_dot(embedding_dir + file)

        #if the node has no embedding, add default embedding 0 array
        for node in G.nodes():
            if "embedding" not in G.nodes[node]:
                G.nodes[node]["embedding"] = [0]*200
            else:
                #delete '[' and ']' in the string
                G.nodes[node]["embedding"] = G.nodes[node]["embedding"].replace('[', '').replace(']', '')
                #delete '"' in the string
                G.nodes[node]["embedding"] = G.nodes[node]["embedding"].replace('"', '')

        #get the feature matrix, the order is the same as the adjacency matrix, same as the node index
        #remove \n and \\n
        F = []
        for node in G.nodes():
            #print(type(G.nodes[node]["embedding"]))
            #change the string to list of numbers
            if type(G.nodes[node]["embedding"]) == str:
                embedding = G.nodes[node]["embedding"].replace('\n', '').replace('\\n', '')
                embedding = list(embedding.split(' '))
                embedding = list(filter(None, embedding))
                #change the string to float
                embedding = [float(i) for i in embedding]
            else:
                embedding = G.nodes[node]["embedding"]
            F.append(embedding)
        feature_matrix.append(F)

        #get the edge_index
        edge_index0 = []
        edge_index1 = []
        for edge in G.edges():
            #change the node name to index
            edge_index0.append(list(G.nodes).index(edge[0]))
            edge_index1.append(list(G.nodes).index(edge[1]))
        edge_index.append([edge_index0, edge_index1]) #dimension: 2*edge_num

    if predict:
        return feature_matrix, edge_index, name

    #get the label
    label_file = pd.read_csv(subDetector().config.path.label)
    unmap_label =label_file['label']
    filenames = label_file['filename']
        
    label = []
    maps = {'malware': 1, 'benignware': 0}
    unmap_label = unmap_label.map(maps)

    #find filname's index and map the label
    for file in embedding_files:
        #file = file[:-4] #remove .dot
        index = filenames[filenames == file].index[0] 
        label.append(unmap_label[index])

    return feature_matrix, edge_index, label, name

#train 
def train(model, all_data, criterion, optimizer, epoch):
    model.train()
    total_loss = 0
    for data in all_data:
        optimizer.zero_grad()
        output = model(data.x, data.edge_index, data)
        loss = criterion(output, data.y)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print('epoch: ', epoch, ' loss: ', total_loss)
    return total_loss

# machine unlearning (sisa) methods
# shard data to n parts
def shard_data(feature_matrix, edge_index, label, name, n):
    print('sharding data...')
    #shard the data to n parts
    shard_feature_matrix = []
    shard_edge_index = []
    shard_label = []
    shard_name = []

    #get the length of each shard
    length = len(label)//n
    for i in range(n):
        shard_feature_matrix.append(feature_matrix[i*length:(i+1)*length])
        shard_edge_index.append(edge_index[i*length:(i+1)*length])
        shard_label.append(label[i*length:(i+1)*length])
        shard_name.append(name[i*length:(i+1)*length])

    return shard_feature_matrix, shard_edge_index, shard_label, shard_name


# slice shard data to m parts
def slice_shard_data(shard_feature_matrix, shard_edge_index, shard_label, shard_name, m):
    print('slicing shard data...')
    #slice the shard data to m parts
    slice_feature_matrix = []
    slice_edge_index = []
    slice_label = []
    slice_name = []

    length = len(shard_label)//m
    for i in range(m):
        slice_feature_matrix.append(shard_feature_matrix[i*length:(i+1)*length])
        slice_edge_index.append(shard_edge_index[i*length:(i+1)*length])
        slice_label.append(shard_label[i*length:(i+1)*length])
        slice_name.append(shard_name[i*length:(i+1)*length])

    return slice_feature_matrix, slice_edge_index, slice_label, slice_name


# train the model, one model for each shard
def sisa_train(slice_feature_matrix, slice_edge_index, slice_label, slice_name, shard_index, m, load = False):
    print('sisa training...')
    #create models directory
    os.makedirs(subDetector().config.folder.model, exist_ok=True)
    os.makedirs(subDetector().config.folder.model + 'slice_models', exist_ok=True)
    os.makedirs(subDetector().config.folder.model + 'final_models', exist_ok=True)

    #construct the model
    model = MalwareExpert(
        input_dim=200,
        hidden_dim=subDetector().config.model.hidden_dim
    )
    if load:
        model.load_state_dict(torch.load(subDetector().config.folder.model + f'final_models/model_{shard_index}.pth'))
        print('load: '+subDetector().config.folder.model + f'final_models/model_{shard_index}.pth')
        
    #construct the loss function
    criterion = nn.CrossEntropyLoss()
    #construct the optimizer
    optimizer = optim.Adam(model.parameters(), lr=subDetector().config.model.learning_rate)

    #train the model, one model for each shard, increase one slice every 'slice_epoch' epochs
    slice_epoch = subDetector().config.model.slice_epoch
    for epoch in range(m*slice_epoch):
        start = time.time()
        if epoch % slice_epoch == 0:

            #concatenate the feature matrix to one list
            feature_matrix = slice_feature_matrix[:int(epoch/slice_epoch)+1]
            feature_matrix = [item for sublist in feature_matrix for item in sublist]
            x = [torch.tensor(feature_matrix[i], dtype=torch.float) for i in range(len(feature_matrix))]

            #concatenate the edge index to one list
            edge_index = slice_edge_index[:int(epoch/slice_epoch)+1]
            edge_index = [item for sublist in edge_index for item in sublist]
            edge_index = [torch.tensor(edge_index[i], dtype=torch.long) for i in range(len(edge_index))]

            #concatenate the label to one list
            label = slice_label[:int(epoch/slice_epoch)+1]
            label = [item for sublist in label for item in sublist]
            y = [torch.tensor(label[i], dtype=torch.long) for i in range(len(label))]

            #concatenate the name to one list
            name = slice_name[:int(epoch/slice_epoch)+1]
            name = [item for sublist in name for item in sublist]

            all_data = [torch_geometric.data.Data(x=x[i], edge_index=edge_index[i], y=y[i], name=name[i]) for i in range(len(x))]
            #train 0.9, val 0.1
            train_data = all_data[:int(len(all_data)*0.9)]
            train_data = torch_geometric.data.Batch.from_data_list(train_data)
            train_data = torch_geometric.loader.DataLoader(train_data, batch_size=subDetector().config.model.batch_size, shuffle=True)
            loss = train(model, train_data, criterion, optimizer, epoch)
            print(all_data)
            print('time: ', time.time()-start)
        else:
            loss = train(model, train_data, criterion, optimizer, epoch)
            print(all_data)
            print('time: ', time.time()-start)

        #validate the model
        #get validation data, 0.1 of the data
        val_data = all_data[int(len(all_data)*0.9):]
        val_data = torch_geometric.data.Batch.from_data_list(val_data)
        val_data = torch_geometric.loader.DataLoader(val_data, batch_size=subDetector().config.model.batch_size, shuffle=True)
        for data in val_data:
            model.eval()
            with torch.no_grad():
                output = model(data.x, data.edge_index, data)
                pred = output.argmax(dim=1)
                correct = pred.eq(data.y).sum().item()
                val_loss = criterion(output, data.y)
                print('val_accuracy: ', correct/len(data.y), 'val_loss: ', val_loss.item())

        #save the model every slice_epoch epochs
        if (epoch+1) % slice_epoch == 0:
            torch.save(model.state_dict(), subDetector().config.folder.model + f'slice_models/model_{shard_index}_{epoch//slice_epoch}.pth')
            print('save: '+subDetector().config.folder.model + f'slice_models/model_{shard_index}_{epoch//slice_epoch}.pth')

    torch.save(model.state_dict(), subDetector().config.folder.model + f'final_models/model_{shard_index}.pth')
    print('save: '+subDetector().config.folder.model + f'final_models/model_{shard_index}.pth')


# use label-based majority vote to predict the label of the test data
def majority_vote(test_data):
    print('majority vote...')
    model = MalwareExpert(
        input_dim=200,
        hidden_dim=subDetector().config.model.hidden_dim
    )
        
    predict = [] #store the prediction of each model
        
    models = os.listdir(subDetector().config.folder.model + 'final_models')
    for test_model in models:
        model.load_state_dict(torch.load(subDetector().config.folder.model + 'final_models/' + test_model))
        model.eval()
        with torch.no_grad():
            output = model(test_data.x, test_data.edge_index, test_data)
            pred = output.argmax(dim=1)
            predict.append(pred.tolist())

    print(predict)

    #majority vote
    malware_count = [0]*len(predict[0])
    benignware_count = [0]*len(predict[0])

    for i in range(len(predict)):
        for j in range(len(predict[i])):
            if predict[i][j] == 1:
                malware_count[j] += 1
            else:
                benignware_count[j] += 1

    final_predict = []
    for i in range(len(malware_count)):
        if malware_count[i] >= benignware_count[i]:
            final_predict.append(1)
        else:
            final_predict.append(0)

    return final_predict

    
# unlearn a data and retrain the model
def unlearn_data(feature_matrix, edge_index, label, name, shard_index, slice_index, m):
    print('unlearning...')
    """#delete the unlearn data from the slice data
    feature_matrix[slice_index].remove(feature_matrix[slice_index][index_in_slice])
    edge_index[slice_index].remove(edge_index[slice_index][index_in_slice])
    label[slice_index].remove(label[slice_index][index_in_slice])
    name[slice_index].remove(name[slice_index][index_in_slice])"""

    #load previous model before the unlearn data
    model = MalwareExpert(
        input_dim=200,
        hidden_dim=subDetector().config.model.hidden_dim
    )
    #if the slice_index is 0, train a new model
    if slice_index == 0:
        print('train a new model')
    else:
        model.load_state_dict(torch.load(subDetector().config.folder.model + f'slice_models/model_{shard_index}_{slice_index-1}.pth'))
        print('load: '+subDetector().config.folder.model + f'slice_models/model_{shard_index}_{slice_index-1}.pth')

    #construct the loss function
    criterion = nn.CrossEntropyLoss()

    #construct the optimizer
    optimizer = optim.Adam(model.parameters(), lr=subDetector().config.model.learning_rate)

    #train the model start from (slice_index-1)
    slice_epoch = subDetector().config.model.slice_epoch
    for epoch in range(slice_index*slice_epoch, m*slice_epoch):
        start = time.time()
        if epoch % slice_epoch == 0:

            #concatenate the feature matrix to one list
            temp_feature_matrix = feature_matrix[:int(epoch/slice_epoch)+1]
            temp_feature_matrix = [item for sublist in temp_feature_matrix for item in sublist]
            temp_x = [torch.tensor(temp_feature_matrix[i], dtype=torch.float) for i in range(len(temp_feature_matrix))]

            #concatenate the edge index to one list
            temp_edge_index = edge_index[:int(epoch/slice_epoch)+1]
            temp_edge_index = [item for sublist in temp_edge_index for item in sublist]
            temp_edge_index = [torch.tensor(temp_edge_index[i], dtype=torch.long) for i in range(len(temp_edge_index))]

            #concatenate the label to one list
            temp_label = label[:int(epoch/slice_epoch)+1] #label is a list of list
            temp_label = [item for sublist in temp_label for item in sublist]
            temp_y = [torch.tensor(temp_label[i], dtype=torch.long) for i in range(len(temp_label))]

            #concatenate the name to one list
            temp_name = name[:int(epoch/slice_epoch)+1]
            temp_name = [item for sublist in temp_name for item in sublist]

            #print('x: ', x)
            #print('edge_index: ', edge_index)
            #print('y: ', y)
            #print('name: ', name)
            print('len(x): ', len(temp_x))
            print('len(edge_index): ', len(temp_edge_index))
            print('len(y): ', len(temp_y))
            print('len(name): ', len(temp_name))

            all_data = [torch_geometric.data.Data(x=temp_x[i], edge_index=temp_edge_index[i], y=temp_y[i], name=temp_name[i]) for i in range(len(temp_x))]
            #train 0.9, val 0.1
            train_data = all_data[:int(len(all_data)*0.9)]
            train_data = torch_geometric.data.Batch.from_data_list(train_data)
            train_data = torch_geometric.loader.DataLoader(train_data, batch_size=subDetector().config.model.batch_size, shuffle=True)
            loss = train(model, train_data, criterion, optimizer, epoch)
            print(all_data)
            print('time: ', time.time()-start)
        else:
            loss = train(model, train_data, criterion, optimizer, epoch)
            print(all_data)
            print('time: ', time.time()-start)

        #validate the model
        #get validation data, 0.1 of the data
        val_data = all_data[int(len(all_data)*0.9):]
        val_data = torch_geometric.data.Batch.from_data_list(val_data)
        val_data = torch_geometric.loader.DataLoader(val_data, batch_size=32, shuffle=True)
        for data in val_data:
            model.eval()
            with torch.no_grad():
                output = model(data.x, data.edge_index, data)
                pred = output.argmax(dim=1)
                correct = pred.eq(data.y).sum().item()
                val_loss = criterion(output, data.y)
                print('val_accuracy: ', correct/len(data.y), 'val_loss: ', val_loss.item())
        
        #save the model every slice_epoch epochs
        if (epoch+1) % slice_epoch == 0:
            torch.save(model.state_dict(), subDetector().config.folder.model + f'slice_models/model_{shard_index}_{epoch//slice_epoch}.pth')
            print('save: '+subDetector().config.folder.model + f'slice_models/model_{shard_index}_{epoch//slice_epoch}.pth')

    torch.save(model.state_dict(), subDetector().config.folder.model + f'final_models/model_{shard_index}.pth')
    print('save: '+subDetector().config.folder.model + f'final_models/model_{shard_index}.pth')
    

# load data shard to unlearn
def load_shard_data(shard_index):
    #get the data names to load (same shard, same slice or after the slice)
    data = []
    with open(subDetector().config.path.record, 'r') as f:
        csv_reader = csv.reader(f)
        next(csv_reader) #skip the header
        for row in csv_reader:
            if int(row[1]) == shard_index:
                data.append(row[0])
            elif int(row[1]) > shard_index:
                break
    
    #load the data
    feature_matrix, edge_index, label, name = load_data(data)
    return feature_matrix, edge_index, label, name


#explain the models
def GNNexplain(test_data):
    print('GNNexplainer...')
    os.makedirs(subDetector().config.folder.explain, exist_ok=True)
    os.makedirs(subDetector().config.folder.explain + 'GNNexplainer', exist_ok=True)

    model = MalwareExpert(200, subDetector().config.model.hidden_dim)
    for test_model in os.listdir(subDetector().config.folder.model + 'final_models'):
        
        os.makedirs(subDetector().config.folder.explain + f'GNNexplainer/{test_model}', exist_ok=True)
        
        model.load_state_dict(torch.load(subDetector().config.folder.model + 'final_models/' + test_model))
        model.eval()

        #use GNNExplainer to explain the model
        explainer = Explainer(
            model=model,
            algorithm=GNNExplainer(epochs=100),
            explanation_type='model',
            node_mask_type='attributes',
            edge_mask_type='object',
            model_config=ModelConfig(
                mode='binary_classification',
                task_level='graph',
                return_type='probs'
            )
        )

        #explanation
        for data in test_data:
            explanation = explainer(
                x=data.x,
                edge_index=data.edge_index,
                data=data
            )
            #print the explanation and visualize the graph
            print(explanation)
            explanation.visualize_graph(path=subDetector().config.folder.explain + f'GNNexplainer/{test_model}/graph_{data.name}.png')
            G = explanation.get_explanation_subgraph()
            print(G)
            G.visualize_graph(path=subDetector().config.folder.explain + f'GNNexplainer/{test_model}/subgraph_{data.name}.png')


# explain the models using graph purning
# edge prunning, input the graph data, output the pruned graph
def edge_pruning(test_data):
    print('edge pruning...')
    os.makedirs(subDetector().config.folder.explain, exist_ok=True)
    os.makedirs(subDetector().config.folder.explain + 'edge_prune', exist_ok=True)

    model = MalwareExpert(200, subDetector().config.model.hidden_dim)
    for test_model in os.listdir(subDetector().config.folder.model + 'final_models'):
        model.load_state_dict(torch.load(subDetector().config.folder.model + 'final_models/' + test_model))

        os.makedirs(subDetector().config.folder.explain + f'edge_prune/{test_model}', exist_ok=True)

        for data in test_data:
            #model with original graph
            model.eval()
            with torch.no_grad():
                output = model(data.x, data.edge_index, data) #output is the probability of being benign
                benign_score = output[0][0].item() #get the probability of being benign
            print('benign_score: ', benign_score)
            print('original edge index: ', data.edge_index)

            edge_to_remove = []
            #prune the graph using edge prunning
            for i in range(len(data.edge_index[0])):
                print('i: ', i)
                temp0 = torch.cat((data.edge_index[0][:i], data.edge_index[0][i+1:]))
                temp1 = torch.cat((data.edge_index[1][:i], data.edge_index[1][i+1:]))
                temp_edge_index = torch.stack((temp0, temp1))
                print('temp_edge_index: ', temp_edge_index)

                temp_data = torch_geometric.data.Data(x=data.x, edge_index=temp_edge_index, y=data.y, name=data.name)
                model.eval()
                with torch.no_grad():
                    output = model(temp_data.x, temp_data.edge_index, temp_data)
                    temp_benign_score = output[0][0].item()
                    print('temp_benign_score: ', temp_benign_score)
                    if abs(temp_benign_score - benign_score) < 0.1: #if the score is not changed much, remove the edge, because the edge is not important
                        edge_to_remove.append(i)

            print('edge to remove: ', edge_to_remove)

            #remove the edge and visualize the pruned graph
            G = nx.Graph()
            for i in range(len(data.edge_index[0])):
                if i not in edge_to_remove:
                    G.add_edge(data.edge_index[0][i].item(), data.edge_index[1][i].item())
            nx.draw(G, with_labels=True)
            plt.show()
            plt.savefig(subDetector().config.folder.explain + f'edge_prune/{test_model}/{data.name}.png')

        
# node prunning, input the graph data, output the pruned graph
def node_pruning(test_data):
    print('node pruning...')
    os.makedirs(subDetector().config.folder.explain, exist_ok=True)
    os.makedirs(subDetector().config.folder.explain + 'node_prune', exist_ok=True)

    model = MalwareExpert(200, subDetector().config.model.hidden_dim)
    for test_model in os.listdir(subDetector().config.folder.model + 'final_models'):
        model.load_state_dict(torch.load(subDetector().config.folder.model + 'final_models/' + test_model))

        os.makedirs(subDetector().config.folder.explain + f'node_prune/{test_model}', exist_ok=True)

        for data in test_data:
            #model with original graph
            model.eval()
            with torch.no_grad():
                output = model(data.x, data.edge_index, data)
                benign_score = output[0][0].item()
            print('benign_score: ', benign_score)
            print('original x: ', data.x)
            print('original edge index: ', data.edge_index)

            node_to_remove = []
            #prune the graph using node prunning, remove node and its edges
            for i in range(len(data.x)):
                print('i: ', i)
                temp_x = data.x.clone().detach()
                temp_x = torch.cat((temp_x[:i], temp_x[i+1:]))
                print('temp_x: ', temp_x)

                #remove the edges that connect to the removed node
                temp0 = []
                temp1 = []
                for j in range(len(data.edge_index[0])):
                    if data.edge_index[0][j] != i and data.edge_index[1][j] != i:
                        temp0.append(data.edge_index[0][j] if data.edge_index[0][j] < i else data.edge_index[0][j]-1)
                        temp1.append(data.edge_index[1][j] if data.edge_index[1][j] < i else data.edge_index[1][j]-1)
                temp0 = torch.tensor(temp0, dtype=torch.long)
                temp1 = torch.tensor(temp1, dtype=torch.long)
                temp_edge_index = torch.stack((temp0, temp1))
                print('temp_edge_index: ', temp_edge_index)

                temp_data = torch_geometric.data.Data(x=temp_x, edge_index=temp_edge_index, y=data.y, name=data.name)
                model.eval()
                with torch.no_grad():
                    output = model(temp_data.x, temp_data.edge_index, temp_data)
                    temp_benign_score = output[0][0].item()
                    print('temp_benign_score: ', temp_benign_score)
                    if abs(temp_benign_score - benign_score) < 0.1: #if the score is not changed much, remove the node, because the node is not important
                        node_to_remove.append(i)

            #remove the node and its edges and visualize the pruned graph
            G = nx.Graph()
            for i in range(len(data.edge_index[0])):
                if data.edge_index[0][i] not in node_to_remove and data.edge_index[1][i] not in node_to_remove:
                    G.add_edge(data.edge_index[0][i].item(), data.edge_index[1][i].item())
            nx.draw(G, with_labels=True)
            plt.show()
            plt.savefig(subDetector().config.folder.explain + f'node_prune/{test_model}/{data.name}.png')

    
