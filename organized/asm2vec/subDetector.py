from malwareDetector.detector import detector
from typing import Any
import numpy as np
import os
import subprocess
from scripts import bin2asm
from scripts import test
import r2pipe
import networkx as nx
from gnn.model import MalwareExpert
from gnn.utils import load_data, train
import torch_geometric
import torch
from time import time

class subDetector(detector):
    def __init__(self) -> None:
        super().__init__()
        self.train_data = []
        #test_data = []
        #self.predict_data = []
        #self.unlearn_data = []
        self.shard_index = 0
        self.slice_index = 0
        self.model_params = None

    def extractFeature(self, purpose:str='train') -> Any:
        print('Extracting features from the dataset...')
        # run scripts/bin2asm.py to extract features
        #subprocess.call("python scripts/bin2asm.py", shell=True)
        if purpose == 'train':
            current_data = self.train_data
            input_path = subDetector().config.folder.dataset
        elif purpose == 'test':
            current_data = self.test_data
            input_path = subDetector().config.folder.dataset
        elif purpose == 'predict':
            current_data = self.predict_data
            input_path = subDetector().config.folder.predict

        os.makedirs(subDetector().config.folder.feature, exist_ok=True)
        print('input path:', input_path)
        print('output path:', subDetector().config.folder.feature)

        for data in current_data:
            data = os.path.join(input_path, data)
            bin2asm.cli(data, subDetector().config.folder.feature, 3)
        return None

    def vectorize(self, purpose:str='train') -> np.array:
        print('Vectorizing the features...')
        # run scripts/asm2vec.py to vectorize the features
        #subprocess.call("python scripts/test.py", shell=True)
        #numpy.set_printoptions(threshold=sys.maxsize)
        os.makedirs(subDetector().config.folder.vectorize, exist_ok=True)
            
        ipath = subDetector().config.folder.feature #asm files directory
        mpath = subDetector().config.path.asm2vec_model #model path

        if purpose == 'train':
            current_data = self.train_data
            dataset = subDetector().config.folder.dataset #binary files
        elif purpose == 'test':
            current_data = self.test_data
            dataset = subDetector().config.folder.dataset
        elif purpose == 'predict':
            current_data = self.predict_data
            dataset = subDetector().config.folder.predict

        print(f'ipath: {ipath}')
        print(f'mpath: {mpath}')
        print(f'dataset: {dataset}')
        print(f'vectorize: {subDetector().config.folder.vectorize}')

        for folder in current_data:  #asm files directory
            dir = os.path.join(ipath, folder)

            #for dir in os.listdir(temp_dir): #asm files (because train part, one more directory level)
            #dir = os.path.join(temp_dir, dir)
            print(f'now vectorize: {dir}')
            for file in current_data: #binary files
                data = os.path.join(dataset, file)
                dir_temp = dir.split('/')[-1]
                data_temp = data.split('/')[-1]
                if dir_temp == data_temp:
                    r2 = r2pipe.open(data)
                    r2.cmd("aaa")
                    r2.cmd("agCd > output.dot")

                    # netwokx graph from dot string
                    G = nx.nx_pydot.read_dot("output.dot")
                    try:
                        for file in os.listdir(dir):
                            file = os.path.join(dir, file)
                            offset, embedding = test.cli(file, mpath)

                            offset_temp = str(offset)[2:]
                            for i in range(8-len(offset_temp)):
                                offset_temp = '0' + offset_temp
                            offset_temp = '0x' + offset_temp

                            #find the node in the graph and add the embedding to the node
                            if G.has_node(offset_temp):
                                G.nodes[offset_temp]['embedding'] = embedding
                    except:
                        with open("error.txt", "a") as f:
                            f.write(dir + '\n')
                        print(f'error in {dir}')

                    #output the graph
                    nx.nx_pydot.write_dot(G, subDetector().config.folder.vectorize+"{}.dot".format(data_temp))
                    break



    def model(self, training:bool=True) -> Any:
        if training:
            # a normal training process, no sisa
            print('Training...')
            start = time()
            #subprocess.run(['python', 'gnn/train.py'])
            # load data
            feature_matrix, edge_index, label, name = load_data(embedding_files=self.train_data)
            x = [torch.tensor(feature_matrix[i], dtype=torch.float) for i in range(len(feature_matrix))]
            edge_index = [torch.tensor(edge_index[i], dtype=torch.long) for i in range(len(edge_index))]
            y = [torch.tensor(label[i], dtype=torch.long) for i in range(len(label))]
            name = [name[i] for i in range(len(name))]
            all_data = [torch_geometric.data.Data(x=x[i], edge_index=edge_index[i], y=y[i], name=name[i]) for i in range(len(x))]

            # create model
            # if model is first slice(model_params=None), create a new model
            # else load the previous slice model(use model_params)
            if self.model_params is None:
                model = MalwareExpert(
                    input_dim=200,
                    hidden_dim=subDetector().config.model.hidden_dim
                )
            else:
                model = self.model_params

            # criterion and optimizer
            criterion = torch.nn.CrossEntropyLoss()
            optimizers = torch.optim.Adam(model.parameters(), lr=subDetector().config.model.learning_rate)

            #train 0.9
            train_data = all_data[:int(len(all_data)*0.9)]
            train_data = torch_geometric.data.Batch.from_data_list(train_data)
            train_data = torch_geometric.loader.DataLoader(train_data, batch_size=subDetector().config.model.batch_size, shuffle=True)
            for epoch in range(subDetector().config.model.epoch):
                train_loss = train(model, train_data, criterion, optimizers, epoch)
                print(all_data)
                print('time: ', time()-start)

            #val 0.1
            val_data = all_data[int(len(all_data)*0.9):]
            val_data = torch_geometric.data.Batch.from_data_list(val_data)
            val_data = torch_geometric.loader.DataLoader(val_data, batch_size=subDetector().config.model.batch_size, shuffle=True)
            for data in val_data:
                model.eval()
                with torch.no_grad():
                    output = model(data.x, data.edge_index, data)
                    pred = output.argmax(dim=1)
                    correct = pred.eq(data.y).sum().item()
                    val_loss = criterion(output, data.y)
                    print('val_accuracy: ', correct/len(data.y), 'val_loss: ', val_loss.item())
            
            #save the model
            self.model_params = model

        else:
            #print('Load model and train...')
            #subprocess.run(['python', 'gnn/load.py'])
            # load model for later to predict
            print('Load model...')
            return self.model_params

    def predict(self) -> np.array:
        # normal prediction process, no majority vote
        print('Predicting the dataset...')
        #subprocess.run(['python', 'gnn/predict.py'])

    def explain(self) -> Any:
        print('Explaining the model...')
        #subprocess.run(['python', 'gnn/explain.py'])

    """def machine_unlearning(self) -> Any:
        print('Machine unlearning...')
        subprocess.run(['python', 'gnn/unlearn.py'])"""

"""if __name__ == '__main__':
    myDetector = subDetector()
    #myDetector.extractFeature()
    #print('Training...')
    #os.system('python scripts/train.py')
    #subprocess.call("python scripts/train.py", shell=True)
    myDetector.vectorize()
    #myDetector.model(training=True)
    #myDetector.model(training=False)
    #myDetector.predict()
    #myDetector.explain()
    #myDetector.machine_unlearning()"""
    