from malwareDetector.detector import detector
from typing import Any
import numpy as np
import os
from scripts import bin2asm
from scripts import test
import r2pipe
import networkx as nx
from gnn.model import MalwareExpert
from gnn.utils import load_data, train, GNNexplain, edge_pruning, node_pruning
import torch_geometric
import torch
from time import time

class subDetector(detector):
    def __init__(self) -> None:
        super().__init__()
        self.train_data = []
        self.test_data = []
        self.predict_data = []
        self.unlearn_data = []
        self.shard_index = 0
        self.slice_index = 0
        self.model_params = None
        self.train_loss = []
        self.val_loss = []

    def extractFeature(self, purpose:str='train') -> Any:
        print('Extracting features from the dataset...')
        if purpose == 'train':
            current_data = self.train_data
            input_path = subDetector().config.folder.dataset
        elif purpose == 'test':
            current_data = self.test_data
            input_path = subDetector().config.folder.dataset
        elif purpose == 'predict':
            current_data = self.predict_data
            input_path = subDetector().config.folder.predict

        os.makedirs(subDetector().config.folder.feature, exist_ok=True)
        print('input path:', input_path)
        print('output path:', subDetector().config.folder.feature)

        for data in current_data:
            #if data already extracted, skip
            if os.path.exists(subDetector().config.folder.feature + data):
                print(f'{data} already extracted')
                continue
            else:
                data = os.path.join(input_path, data)
                bin2asm.cli(data, subDetector().config.folder.feature, 3)
        return None

    def vectorize(self, purpose:str='train') -> np.array:
        print('Vectorizing the features...')
        os.makedirs(subDetector().config.folder.vectorize, exist_ok=True)
            
        ipath = subDetector().config.folder.feature #asm files directory
        mpath = subDetector().config.path.asm2vec_model #model path

        if purpose == 'train':
            current_data = self.train_data
            dataset = subDetector().config.folder.dataset #binary files
        elif purpose == 'test':
            current_data = self.test_data
            dataset = subDetector().config.folder.dataset
        elif purpose == 'predict':
            current_data = self.predict_data
            dataset = subDetector().config.folder.predict

        print(f'ipath: {ipath}')
        print(f'mpath: {mpath}')
        print(f'dataset: {dataset}')
        print(f'vectorize: {subDetector().config.folder.vectorize}')

        for folder in current_data:  #asm files directory
            dir = os.path.join(ipath, folder)
            #if dir already vectorized, skip
            if os.path.exists(subDetector().config.folder.vectorize + folder + '.dot'):
                print(f'{dir} already vectorized')
                continue
            else:
                print(f'now vectorize: {dir}')
                for file in current_data: #binary files
                    data = os.path.join(dataset, file)
                    dir_temp = dir.split('/')[-1]
                    data_temp = data.split('/')[-1]
                    if dir_temp == data_temp:
                        r2 = r2pipe.open(data)
                        r2.cmd("aaa")
                        r2.cmd("agCd > output.dot")

                        # netwokx graph from dot string
                        G = nx.nx_pydot.read_dot("output.dot")
                        try:
                            for file in os.listdir(dir):
                                file = os.path.join(dir, file)
                                offset, embedding = test.cli(file, mpath)

                                offset_temp = str(offset)[2:]
                                for i in range(8-len(offset_temp)):
                                    offset_temp = '0' + offset_temp
                                offset_temp = '0x' + offset_temp

                                #find the node in the graph and add the embedding to the node
                                if G.has_node(offset_temp):
                                    G.nodes[offset_temp]['embedding'] = embedding
                        except:
                            with open("error.txt", "a") as f:
                                f.write(dir + '\n')
                            print(f'error in {dir}')

                        #output the graph
                        nx.nx_pydot.write_dot(G, subDetector().config.folder.vectorize+"{}.dot".format(data_temp))
                        break

    def model(self, training:bool=True) -> Any:
        print(f'current model: model_{self.shard_index}_{self.slice_index}.pt')
        if training:
            # a normal training process, no sisa
            print('Training...')
            start = time()
            # load data
            feature_matrix, edge_index, label, name = load_data(embedding_files=self.train_data)
            x = [torch.tensor(feature_matrix[i], dtype=torch.float) for i in range(len(feature_matrix))]
            edge_index = [torch.tensor(edge_index[i], dtype=torch.long) for i in range(len(edge_index))]
            y = [torch.tensor(label[i], dtype=torch.long) for i in range(len(label))]
            name = [name[i] for i in range(len(name))]
            all_data = [torch_geometric.data.Data(x=x[i], edge_index=edge_index[i], y=y[i], name=name[i]) for i in range(len(x))]

            # create model
            if self.model_params == None:
                model = MalwareExpert(200, subDetector().config.model.hidden_dim)
            else:
                model = self.model_params

            # criterion and optimizer
            criterion = torch.nn.CrossEntropyLoss()
            optimizers = torch.optim.Adam(model.parameters(), lr=subDetector().config.model.learning_rate)

            #train 0.9
            train_data = all_data[:int(len(all_data)*0.9)]
            train_data = torch_geometric.data.Batch.from_data_list(train_data)
            train_data = torch_geometric.loader.DataLoader(train_data, batch_size=subDetector().config.model.batch_size, shuffle=True)
            #val 0.1
            val_data = all_data[int(len(all_data)*0.9):]
            val_data = torch_geometric.data.Batch.from_data_list(val_data)
            val_data = torch_geometric.loader.DataLoader(val_data, batch_size=subDetector().config.model.batch_size, shuffle=True)
            for epoch in range(subDetector().config.model.epoch):
                train_loss = train(model, train_data, criterion, optimizers)
                print(all_data)
                print('time: ', time()-start)
                print('epoch: ', epoch, ' loss: ', train_loss)
                self.train_loss.append(train_loss)
                # validation every epoch
                val_loss = 0
                model.eval()
                with torch.no_grad():
                    for data in val_data:
                        output = model(data.x, data.edge_index, data)
                        pred = output.argmax(dim=1)
                        correct = pred.eq(data.y).sum().item()
                        loss = criterion(output, data.y)
                        val_loss += loss.item()
                    print('val_accuracy: ', correct/len(data.y), 'val_loss: ', val_loss)
                    self.val_loss.append(val_loss)
            
            #save the model
            self.model_params = model

        else:
            # load model for later use
            print('Load model...')
            self.model_params = MalwareExpert(200, subDetector().config.model.hidden_dim)
            self.model_params.load_state_dict(torch.load(subDetector().config.folder.model + f'slice_models/model_{self.shard_index}_{self.slice_index}.pt'))
            return self.model_params

    def predict(self, purpose:str='test') -> np.array:
        # normal prediction process, no majority vote
        print('Predicting the dataset...')
        if purpose == 'test':
            current_data = self.test_data
        elif purpose == 'predict':
            current_data = self.predict_data

        # load data
        feature_matrix, edge_index, name = load_data(embedding_files=current_data, predict=True)
        x = [torch.tensor(feature_matrix[i], dtype=torch.float) for i in range(len(feature_matrix))]
        edge_index = [torch.tensor(edge_index[i], dtype=torch.long) for i in range(len(edge_index))]
        name = name
        all_data = [torch_geometric.data.Data(x=x[i], edge_index=edge_index[i], name=name[i]) for i in range(len(x))]

        # predict the dataset
        predict = [] #list of predicted labels
        for data in all_data:
            model = self.model_params
            model.eval()
            with torch.no_grad():
                output = model(data.x, data.edge_index, data)
                pred = output.argmax(dim=1)
                predict.append(pred.item())
        
        print('Predicted labels:', predict)
        return predict

    def explain(self) -> Any:
        print('Explaining the model...')
        current_data = self.predict_data #(or explain_data)

        # load data
        feature_matrix, edge_index, name = load_data(embedding_files=current_data, predict=True)
        x = [torch.tensor(feature_matrix[i], dtype=torch.float) for i in range(len(feature_matrix))]
        edge_index = [torch.tensor(edge_index[i], dtype=torch.long) for i in range(len(edge_index))]
        name = name
        all_data = [torch_geometric.data.Data(x=x[i], edge_index=edge_index[i], name=name[i]) for i in range(len(x))]
        
        #explain the model
        model = self.model_params
        GNNexplain(model, self.shard_index, self.slice_index, all_data)
        edge_pruning(model, self.shard_index, self.slice_index, all_data)
        node_pruning(model, self.shard_index, self.slice_index, all_data)

    