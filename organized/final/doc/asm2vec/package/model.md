# ASM2VEC Model Documentation

## Overview

The `model.py` file contains the implementation of the ASM2VEC model, which is designed to learn embeddings of assembly code. This model is based on the idea of representing assembly instructions and functions as dense vectors, allowing for various downstream tasks such as similarity comparison, classification, or anomaly detection.

## Class: ASM2VEC

The main class in this module is `ASM2VEC`, which inherits from `nn.Module`. This class implements the neural network architecture for learning assembly code embeddings.

### Initialization

```python
def __init__(self, vocab_size: int, function_size: int, embedding_size: int):
```

- `vocab_size`: The size of the token vocabulary.
- `function_size`: The number of unique functions.
- `embedding_size`: The dimension of the embedding vectors.

The constructor initializes three embedding layers:
1. `embeddings`: For token embeddings
2. `embeddings_f`: For function embeddings
3. `embeddings_r`: For reverse token embeddings (used in prediction)

### Key Methods

#### update

```python
def update(self, function_size_new: int, vocab_size_new: int):
```

This method updates the model's embeddings to accommodate new vocabulary or functions. It's useful when you want to extend the model to work with new assembly code that contains previously unseen tokens or functions.

#### v

```python
def v(self, inp: torch.Tensor) -> torch.Tensor:
```

Computes the context vector for a given input sequence. This method combines embeddings of the current token, previous tokens, and next tokens to create a context-aware representation.

#### forward

```python
def forward(self, inp: torch.Tensor, pos: torch.Tensor, neg: torch.Tensor) -> torch.Tensor:
```

Implements the forward pass of the model. It uses negative sampling to compute the loss, which is returned as a binary cross-entropy loss.

#### predict

```python
def predict(self, inp: torch.Tensor, pos: torch.Tensor) -> torch.Tensor:
```

Predicts probabilities for tokens given an input sequence. This method is useful for tasks like next-token prediction or assessing the model's understanding of assembly code patterns.

#### get_embedding

```python
def get_embedding(self, inp: torch.Tensor) -> torch.Tensor:
```

Retrieves the embedding of an input function by computing the average of the embeddings of its tokens.

## Usage

To use the ASM2VEC model:

1. Initialize the model with appropriate vocabulary size, function size, and embedding size.
2. Prepare your assembly code data and convert it to the required tensor format.
3. Train the model using the `forward` method and an appropriate optimizer.
4. Use the trained model to get embeddings for new assembly code or make predictions.

Example:

```python
model = ASM2VEC(vocab_size=1000, function_size=100, embedding_size=128)
# ... (prepare data)
optimizer = torch.optim.Adam(model.parameters())
for epoch in range(num_epochs):
    loss = model(input_data, positive_samples, negative_samples)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

After training, you can use the model to get embeddings or make predictions:

```python
embeddings = model.get_embedding(new_function)
predictions = model.predict(input_sequence, positive_samples)
```

## Note

This model is designed specifically for assembly code and may require domain-specific preprocessing of your assembly code data. Ensure that your input data is properly tokenized and formatted before feeding it into the model.
