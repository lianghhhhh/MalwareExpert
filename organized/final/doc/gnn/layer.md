# ASM2VEC Layer Module Documentation

## Overview

The `layer.py` module defines a custom Graph Convolutional Network (GCN) layer called `deeperGCN`. This layer is designed to be used in deeper graph neural network architectures, incorporating normalization, activation, and dropout for improved performance and stability.

## Class: deeperGCN

```python
class deeperGCN(nn.Module):
```

This class implements a deeper Graph Convolutional Network layer.

### Initialization

```python
def __init__(self, in_channels: int, out_channels: int, dropout_value: float):
```

#### Parameters:
- `in_channels` (int): Number of input features.
- `out_channels` (int): Number of output features.
- `dropout_value` (float): Dropout probability.

#### Attributes:
- `norm`: Layer normalization (nn.LayerNorm)
- `act`: ReLU activation function (nn.ReLU)
- `dropout`: Dropout layer (nn.Dropout)
- `conv`: Graph Convolutional layer (GCNConv)

### Forward Pass

```python
def forward(self, x: torch.Tensor, edge_index: Adj) -> torch.Tensor:
```

#### Parameters:
- `x` (torch.Tensor): Input node features of shape [num_nodes, in_channels].
- `edge_index` (Adj): Graph connectivity in COO (coordinate) format with shape [2, num_edges].

#### Returns:
- torch.Tensor: Output node features of shape [num_nodes, out_channels].

#### Process:
1. Apply layer normalization
2. Apply ReLU activation
3. Apply dropout
4. Apply graph convolution

## Usage

This layer is designed to be used as a building block in deeper graph neural network architectures. It can be incorporated into a model as follows:

```python
from layer import deeperGCN

class DeepGNNModel(nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels, dropout):
        super(DeepGNNModel, self).__init__()
        self.conv1 = deeperGCN(in_channels, hidden_channels, dropout)
        self.conv2 = deeperGCN(hidden_channels, out_channels, dropout)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = self.conv2(x, edge_index)
        return x
```

This layer is particularly useful for creating deeper GNN architectures while mitigating issues like vanishing gradients and over-smoothing that can occur in deep graph neural networks.
