from matplotlib import pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
import torch_geometric
import torch_geometric.data
from model import MalwareExpert
from train import train
import time
import os
import networkx as nx
import pandas as pd
from torch_geometric.explain import Explainer, GNNExplainer, ModelConfig

#load data
def load_data():
    graph_dir = '../embedding/'
    graph_files = os.listdir(graph_dir)
    feature_matrix = []
    edge_index = []
    name = []

    for file in graph_files:
        G = nx.nx_pydot.read_dot(graph_dir + file)

        #if the node has no embedding, add default embedding 0 array
        for node in G.nodes():
            if "embedding" not in G.nodes[node]:
                G.nodes[node]["embedding"] = [0]*200
            else:
                #delete '[' and ']' in the string
                G.nodes[node]["embedding"] = G.nodes[node]["embedding"].replace('[', '').replace(']', '')
                #delete '"' in the string
                G.nodes[node]["embedding"] = G.nodes[node]["embedding"].replace('"', '')

        #get the feature matrix, the order is the same as the adjacency matrix, same as the node index
        #remove \n and \\n
        F = []
        for node in G.nodes():
            #print(type(G.nodes[node]["embedding"]))
            #change the string to list of numbers
            if type(G.nodes[node]["embedding"]) == str:
                embedding = G.nodes[node]["embedding"].replace('\n', '').replace('\\n', '')
                embedding = list(embedding.split(' '))
                embedding = list(filter(None, embedding))
                #change the string to float
                embedding = [float(i) for i in embedding]
            F.append(embedding)
        feature_matrix.append(F)

        #get the edge_index
        edge_index0 = []
        edge_index1 = []
        for edge in G.edges():
            #change the node name to index
            edge_index0.append(list(G.nodes).index(edge[0]))
            edge_index1.append(list(G.nodes).index(edge[1]))
        edge_index.append([edge_index0, edge_index1]) #dimension: 2*edge_num


    #get the label
    label_file = pd.read_csv('dataset.csv')
    unmap_label =label_file['label']
    filenames = label_file['filename']
        
    label = []
    maps = {'malware': 1, 'benignware': 0}
    unmap_label = unmap_label.map(maps)

    #find filname's index and map the label
    for file in graph_files:
        file = file[:-4] #remove .dot
        index = filenames[filenames == file].index[0] 
        label.append(unmap_label[index])
        name.append(file)

    return feature_matrix, edge_index, label, name


#load_data()

# machine unlearning (sisa) methods
# shard data to n parts
def shard_data(feature_matrix, edge_index, label, name, n):
    #shard the data to n parts
    shard_feature_matrix = []
    shard_edge_index = []
    shard_label = []
    shard_name = []

    #get the length of each shard
    length = len(label)//n
    for i in range(n):
        shard_feature_matrix.append(feature_matrix[i*length:(i+1)*length])
        shard_edge_index.append(edge_index[i*length:(i+1)*length])
        shard_label.append(label[i*length:(i+1)*length])
        shard_name.append(name[i*length:(i+1)*length])

    return shard_feature_matrix, shard_edge_index, shard_label, shard_name


# slice shard data to m parts
def slice_shard_data(shard_feature_matrix, shard_edge_index, shard_label, shard_name, m):
    #slice the shard data to m parts
    slice_feature_matrix = []
    slice_edge_index = []
    slice_label = []
    slice_name = []

    length = len(shard_label)//m
    for i in range(m):
        slice_feature_matrix.append(shard_feature_matrix[i*length:(i+1)*length])
        slice_edge_index.append(shard_edge_index[i*length:(i+1)*length])
        slice_label.append(shard_label[i*length:(i+1)*length])
        slice_name.append(shard_name[i*length:(i+1)*length])

    return slice_feature_matrix, slice_edge_index, slice_label, slice_name


# train the model, one model for each shard
def sisa_train(slice_feature_matrix, slice_edge_index, slice_label, slice_name, shard_index, m):
    #create models directory
    if not os.path.exists('models'):
        os.makedirs('models')
    if not os.path.exists('final_models'):
        os.makedirs('final_models')

    #construct the model
    model = MalwareExpert(
        input_dim=200,
        hidden_dim=100
    )

    #construct the loss function
    criterion = nn.CrossEntropyLoss()

    #construct the optimizer
    optimizer = optim.Adam(model.parameters(), lr=0.01)

    #train the model, one model for each shard, increase one slice every 10 epochs
    for epoch in range(m*10):
        start = time.time()
        if epoch % 10 == 0:

            #concatenate the feature matrix to one list
            feature_matrix = slice_feature_matrix[:int(epoch/10)+1]
            feature_matrix = [item for sublist in feature_matrix for item in sublist]
            x = [torch.tensor(feature_matrix[i], dtype=torch.float) for i in range(len(feature_matrix))]

            #concatenate the edge index to one list
            edge_index = slice_edge_index[:int(epoch/10)+1]
            edge_index = [item for sublist in edge_index for item in sublist]
            edge_index = [torch.tensor(edge_index[i], dtype=torch.long) for i in range(len(edge_index))]

            #concatenate the label to one list
            label = slice_label[:int(epoch/10)+1]
            label = [item for sublist in label for item in sublist]
            y = [torch.tensor(label[i], dtype=torch.long) for i in range(len(label))]

            #concatenate the name to one list
            name = slice_name[:int(epoch/10)+1]
            name = [item for sublist in name for item in sublist]

            all_data = [torch_geometric.data.Data(x=x[i], edge_index=edge_index[i], y=y[i], name=name[i]) for i in range(len(x))]
            #train 0.9, val 0.1
            train_data = all_data[:int(len(all_data)*0.9)]
            train_data = torch_geometric.data.Batch.from_data_list(train_data)
            train_data = torch_geometric.loader.DataLoader(train_data, batch_size=32, shuffle=True)
            loss = train(model, train_data, criterion, optimizer, epoch)
            print(all_data)
            print('time: ', time.time()-start)
        else:
            loss = train(model, train_data, criterion, optimizer, epoch)
            print(all_data)
            print('time: ', time.time()-start)

        #validate the model
        #get validation data, 0.1 of the data
        val_data = all_data[int(len(all_data)*0.9):]
        val_data = torch_geometric.data.Batch.from_data_list(val_data)
        val_data = torch_geometric.loader.DataLoader(val_data, batch_size=32, shuffle=True)
        for data in val_data:
            model.eval()
            with torch.no_grad():
                output = model(data.x, data.edge_index, data)
                pred = output.argmax(dim=1)
                correct = pred.eq(data.y).sum().item()
                val_loss = criterion(output, data.y)
                print('val_accuracy: ', correct/len(data.y), 'val_loss: ', val_loss.item())

        #save the model every 10 epochs
        if (epoch+1) % 10 == 0:
            torch.save(model.state_dict(), f'models/model_{shard_index}_{epoch//10}.pth')
            print('save: '+f'models/model_{shard_index}_{epoch//10}.pth')

    torch.save(model.state_dict(), f'final_models/model_{shard_index}.pth')
    print('save: '+f'final_models/model_{shard_index}.pth')


# use label-based majority vote to predict the label of the test data
def majority_vote(test_data):

    model = MalwareExpert(
        input_dim=200,
        hidden_dim=100
    )
        
    predict = [] #store the prediction of each model
        
    models = os.listdir('final_models')
    for test_model in models:
        model.load_state_dict(torch.load('final_models/' + test_model))
        model.eval()
        with torch.no_grad():
            output = model(test_data.x, test_data.edge_index, test_data)
            pred = output.argmax(dim=1)
            predict.append(pred.tolist())

    print(predict)

    #majority vote
    malware_count = [0]*len(predict[0])
    benignware_count = [0]*len(predict[0])

    for i in range(len(predict)):
        for j in range(len(predict[i])):
            if predict[i][j] == 1:
                malware_count[j] += 1
            else:
                benignware_count[j] += 1

    final_predict = []
    for i in range(len(malware_count)):
        if malware_count[i] >= benignware_count[i]:
            final_predict.append(1)
        else:
            final_predict.append(0)

    return final_predict

    
# unlearn a data and retrain the model
def unlearn_data(feature_matrix, edge_index, label, name, shard_index, slice_index, index_in_slice, m):
    #delete the unlearn data from the slice data
    feature_matrix[slice_index].remove(feature_matrix[slice_index][index_in_slice])
    edge_index[slice_index].remove(edge_index[slice_index][index_in_slice])
    label[slice_index].remove(label[slice_index][index_in_slice])
    name[slice_index].remove(name[slice_index][index_in_slice])

    #load previous model before the unlearn data
    model = MalwareExpert(
        input_dim=200,
        hidden_dim=100
    )
    #if the slice_index is 0, train a new model
    if slice_index == 0:
        model = MalwareExpert(
            input_dim=200,
            hidden_dim=100
        )
        print('train a new model')
    else:
        model.load_state_dict(torch.load(f'models/model_{shard_index}_{slice_index-1}.pth'))
        print('load: '+f'models/model_{shard_index}_{slice_index-1}.pth')

    #construct the loss function
    criterion = nn.CrossEntropyLoss()

    #construct the optimizer
    optimizer = optim.Adam(model.parameters(), lr=0.01)

    #train the model start from (slice_index-1)
    for epoch in range(slice_index*10, m*10):
        start = time.time()
        if epoch % 10 == 0:

            #concatenate the feature matrix to one list
            temp_feature_matrix = feature_matrix[:int(epoch/10)+1]
            temp_feature_matrix = [item for sublist in temp_feature_matrix for item in sublist]
            temp_x = [torch.tensor(temp_feature_matrix[i], dtype=torch.float) for i in range(len(temp_feature_matrix))]

            #concatenate the edge index to one list
            temp_edge_index = edge_index[:int(epoch/10)+1]
            temp_edge_index = [item for sublist in temp_edge_index for item in sublist]
            temp_edge_index = [torch.tensor(temp_edge_index[i], dtype=torch.long) for i in range(len(temp_edge_index))]

            #concatenate the label to one list
            temp_label = label[:int(epoch/10)+1] #label is a list of list
            temp_label = [item for sublist in temp_label for item in sublist]
            temp_y = [torch.tensor(temp_label[i], dtype=torch.long) for i in range(len(temp_label))]

            #concatenate the name to one list
            temp_name = name[:int(epoch/10)+1]
            temp_name = [item for sublist in temp_name for item in sublist]

            #print('x: ', x)
            #print('edge_index: ', edge_index)
            #print('y: ', y)
            #print('name: ', name)
            print('len(x): ', len(temp_x))
            print('len(edge_index): ', len(temp_edge_index))
            print('len(y): ', len(temp_y))
            print('len(name): ', len(temp_name))

            all_data = [torch_geometric.data.Data(x=temp_x[i], edge_index=temp_edge_index[i], y=temp_y[i], name=temp_name[i]) for i in range(len(temp_x))]
            #train 0.9, val 0.1
            train_data = all_data[:int(len(all_data)*0.9)]
            train_data = torch_geometric.data.Batch.from_data_list(train_data)
            train_data = torch_geometric.loader.DataLoader(train_data, batch_size=32, shuffle=True)
            loss = train(model, train_data, criterion, optimizer, epoch)
            print(all_data)
            print('time: ', time.time()-start)
        else:
            loss = train(model, train_data, criterion, optimizer, epoch)
            print(all_data)
            print('time: ', time.time()-start)

        #validate the model
        #get validation data, 0.1 of the data
        val_data = all_data[int(len(all_data)*0.9):]
        val_data = torch_geometric.data.Batch.from_data_list(val_data)
        val_data = torch_geometric.loader.DataLoader(val_data, batch_size=32, shuffle=True)
        for data in val_data:
            model.eval()
            with torch.no_grad():
                output = model(data.x, data.edge_index, data)
                pred = output.argmax(dim=1)
                correct = pred.eq(data.y).sum().item()
                val_loss = criterion(output, data.y)
                print('val_accuracy: ', correct/len(data.y), 'val_loss: ', val_loss.item())
        
        #save the model every 10 epochs
        if (epoch+1) % 10 == 0:
            torch.save(model.state_dict(), f'models/model_{shard_index}_{epoch//10}.pth')
            print('save: '+f'models/model_{shard_index}_{epoch//10}.pth')

    torch.save(model.state_dict(), f'final_models/model_{shard_index}.pth')
    print('save: '+f'final_models/model_{shard_index}.pth')
    
  
#explain the models
def GNNexplain(test_data):
    if not os.path.exists('graph'):
        os.makedirs('graph')
    if not os.path.exists('graph/GNNexplainer'):
        os.makedirs('graph/GNNexplainer')

    model = MalwareExpert(200, 100)
    for test_model in os.listdir('final_models'):
        
        if not os.path.exists(f'graph/GNNexplainer/{test_model}'):
            os.makedirs(f'graph/GNNexplainer/{test_model}')
        
        model.load_state_dict(torch.load('final_models/' + test_model))
        model.eval()

        #use GNNExplainer to explain the model
        explainer = Explainer(
            model=model,
            algorithm=GNNExplainer(epochs=100),
            explanation_type='model',
            node_mask_type='attributes',
            edge_mask_type='object',
            model_config=ModelConfig(
                mode='binary_classification',
                task_level='graph',
                return_type='probs'
            )
        )

        #explanation
        for data in test_data:
            explanation = explainer(
                x=data.x,
                edge_index=data.edge_index,
                data=data
            )
            #print the explanation and visualize the graph
            print(explanation)
            explanation.visualize_graph(path=f'graph/GNNexplainer/{test_model}/graph_{data.name}.png')
            G = explanation.get_explanation_subgraph()
            print(G)
            G.visualize_graph(path=f'graph/GNNexplainer/{test_model}/subgraph_{data.name}.png')


# explain the models using graph purning
# edge prunning, input the graph data, output the pruned graph
def edge_pruning(test_data):
    if not os.path.exists('graph'):
        os.makedirs('graph')
    if not os.path.exists('graph/edge_prune'):
        os.makedirs('graph/edge_prune')

    model = MalwareExpert(200, 100)
    for test_model in os.listdir('final_models'):
        model.load_state_dict(torch.load('final_models/' + test_model))

        if not os.path.exists(f'graph/edge_prune/{test_model}'):
            os.makedirs(f'graph/edge_prune/{test_model}')

        for data in test_data:
            #model with original graph
            model.eval()
            with torch.no_grad():
                output = model(data.x, data.edge_index, data) #output is the probability of being benign
                benign_score = output[0][0].item() #get the probability of being benign
            print('benign_score: ', benign_score)
            print('original edge index: ', data.edge_index)

            edge_to_remove = []
            #prune the graph using edge prunning
            for i in range(len(data.edge_index[0])):
                print('i: ', i)
                temp0 = torch.cat((data.edge_index[0][:i], data.edge_index[0][i+1:]))
                temp1 = torch.cat((data.edge_index[1][:i], data.edge_index[1][i+1:]))
                temp_edge_index = torch.stack((temp0, temp1))
                print('temp_edge_index: ', temp_edge_index)

                temp_data = torch_geometric.data.Data(x=data.x, edge_index=temp_edge_index, y=data.y, name=data.name)
                model.eval()
                with torch.no_grad():
                    output = model(temp_data.x, temp_data.edge_index, temp_data)
                    temp_benign_score = output[0][0].item()
                    print('temp_benign_score: ', temp_benign_score)
                    if abs(temp_benign_score - benign_score) < 0.1: #if the score is not changed much, remove the edge, because the edge is not important
                        edge_to_remove.append(i)

            print('edge to remove: ', edge_to_remove)

            #remove the edge and visualize the pruned graph
            G = nx.Graph()
            for i in range(len(data.edge_index[0])):
                if i not in edge_to_remove:
                    G.add_edge(data.edge_index[0][i].item(), data.edge_index[1][i].item())
            nx.draw(G, with_labels=True)
            plt.show()
            plt.savefig(f'graph/edge_prune/{test_model}/{data.name}.png')

        
# node prunning, input the graph data, output the pruned graph
def node_pruning(test_data):
    if not os.path.exists('graph'):
        os.makedirs('graph')
    if not os.path.exists('graph/node_prune'):
        os.makedirs('graph/node_prune')

    model = MalwareExpert(200, 100)
    for test_model in os.listdir('final_models'):
        model.load_state_dict(torch.load('final_models/' + test_model))

        if not os.path.exists(f'graph/node_prune/{test_model}'):
            os.makedirs(f'graph/node_prune/{test_model}')

        for data in test_data:
            #model with original graph
            model.eval()
            with torch.no_grad():
                output = model(data.x, data.edge_index, data)
                benign_score = output[0][0].item()
            print('benign_score: ', benign_score)
            print('original x: ', data.x)
            print('original edge index: ', data.edge_index)

            node_to_remove = []
            #prune the graph using node prunning, remove node and its edges
            for i in range(len(data.x)):
                print('i: ', i)
                temp_x = data.x.clone().detach()
                temp_x = torch.cat((temp_x[:i], temp_x[i+1:]))
                print('temp_x: ', temp_x)

                #remove the edges that connect to the removed node
                temp0 = []
                temp1 = []
                for j in range(len(data.edge_index[0])):
                    if data.edge_index[0][j] != i and data.edge_index[1][j] != i:
                        temp0.append(data.edge_index[0][j] if data.edge_index[0][j] < i else data.edge_index[0][j]-1)
                        temp1.append(data.edge_index[1][j] if data.edge_index[1][j] < i else data.edge_index[1][j]-1)
                temp0 = torch.tensor(temp0, dtype=torch.long)
                temp1 = torch.tensor(temp1, dtype=torch.long)
                temp_edge_index = torch.stack((temp0, temp1))
                print('temp_edge_index: ', temp_edge_index)

                temp_data = torch_geometric.data.Data(x=temp_x, edge_index=temp_edge_index, y=data.y, name=data.name)
                model.eval()
                with torch.no_grad():
                    output = model(temp_data.x, temp_data.edge_index, temp_data)
                    temp_benign_score = output[0][0].item()
                    print('temp_benign_score: ', temp_benign_score)
                    if abs(temp_benign_score - benign_score) < 0.1: #if the score is not changed much, remove the node, because the node is not important
                        node_to_remove.append(i)

            #remove the node and its edges and visualize the pruned graph
            G = nx.Graph()
            for i in range(len(data.edge_index[0])):
                if data.edge_index[0][i] not in node_to_remove and data.edge_index[1][i] not in node_to_remove:
                    G.add_edge(data.edge_index[0][i].item(), data.edge_index[1][i].item())
            nx.draw(G, with_labels=True)
            plt.show()
            plt.savefig(f'graph/node_prune/{test_model}/{data.name}.png')

    
