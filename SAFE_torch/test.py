# @inproceedings{massarelli2018safe,
#   title={SAFE: Self-Attentive Function Embeddings for Binary Similarity},
#   author={Massarelli, Luca and Di Luna, Giuseppe Antonio and Petroni, Fabio and Querzoni, Leonardo and Baldoni, Roberto},
#   booktitle={Proceedings of 16th Conference on Detection of Intrusions and Malware & Vulnerability Assessment (DIMVA)},
#   year={2019}
# }

from utils.function_normalizer import FunctionNormalizer
from utils.instructions_converter import InstructionsConverter
from utils.capstone_disassembler import disassemble
from utils.radare_analyzer import BinaryAnalyzer
from safetorch.safe_network import SAFE
from safetorch.parameters import Config
import torch
import r2pipe
import networkx as nx
import os

#import sys
#binary_path = sys.argv[1]
binary_path = 'dataset/'
if not os.path.exists("embedding"):
    os.mkdir("embedding")

for file in os.listdir(binary_path):
    malware = os.path.join(binary_path, file)
    print(malware)

    # create cfg
    r2 = r2pipe.open(malware)
    r2.cmd("aaa")
    r2.cmd("agCd > output.dot")
    # netwokx graph from dot string
    G = nx.nx_pydot.read_dot("output.dot")
    r2.quit()

    # initialize SAFE
    config = Config()
    safe = SAFE(config)

    # load instruction converter and normalizer
    I2V_FILENAME = "model/word2id.json"
    converter = InstructionsConverter(I2V_FILENAME)
    normalizer = FunctionNormalizer(max_instruction=150)

    # load SAFE weights
    SAFE_torch_model_path = "model/SAFEtorch.pt"
    state_dict = torch.load(SAFE_torch_model_path)
    safe.load_state_dict(state_dict)
    safe = safe.eval()

    # analyze the binary
    binary = BinaryAnalyzer(malware)
    offsets = binary.get_functions()

    # generate each function embedding
    for offset in offsets:
        asm = binary.get_hexasm(offset)
        instructions = disassemble(asm, binary.arch, binary.bits)
        converted_instructions = converter.convert_to_ids(instructions)
        instructions, length = normalizer.normalize_functions(
            [converted_instructions])
        tensor = torch.LongTensor(instructions[0])
        function_embedding = safe(tensor, length)

        #change tensor to numpy
        function_embedding = function_embedding.detach().numpy()
        #print(hex(offset), function_embedding)

        offset_temp = str(hex(offset))[2:]
        for i in range(8 - len(offset_temp)):
            offset_temp = '0' + offset_temp
        offset_temp = '0x' + offset_temp

        if G.has_node(offset_temp):
            G.nodes[offset_temp]['embedding'] = function_embedding

        #find the node in the graph and add the embedding to the node

        """
        for node in G.nodes():
            node_temp = str(node)[2:]

            #remove the 0 in the beginning
            for i in range(len(node_temp)):
                if node_temp[0] == '0':
                    node_temp = node_temp[1:]
                else:
                    break
            
            if node_temp == offset_temp:
                G.nodes[node]['embedding'] = function_embedding
                break
        """
    #output the graph
    nx.nx_pydot.write_dot(G, "embedding/{}.dot".format(file))
