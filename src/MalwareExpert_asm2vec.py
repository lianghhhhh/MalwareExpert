import os
import torch
import r2pipe
import logging
import numpy as np
import networkx as nx
import torch_geometric
from time import time
from typing import Any
from gnn.utils import MalwareAnalyzer
from gnn.model import MalwareExpertModel
from asm2vec.scripts import test, bin2asm
from malwareDetector.detector import detector
from malwareDetector.config import read_config

class MalwareExpert_asm2vec(detector):
    """
    A class for malware detection using ASM2Vec and Graph Neural Networks.
    """
    def __init__(self, logger=None, config_path: str=None) -> None:
        """
        Initialize the MalwareExpert_asm2vec detector.

        Args:
            logger: Logger object. If None, a new logger will be created.
            config_path (str): Path to the configuration file.
        """
        self.config = read_config(config_path)

        if logger is None:
            self.logger = self.set_log(self.config.path.log)
        else:
            self.logger = logger

        self.malware_analyzer = MalwareAnalyzer(self.logger, self.config)
        self.train_data = []
        self.test_data = []
        self.predict_data = []
        self.unlearn_data = []
        self.shard_index = 0
        self.slice_index = 0
        self.model_params = None
        self.train_loss = []
        self.val_loss = []

    def set_log(log_path: str=None) -> logging.Logger:
        """
            Set up and configure the logger for the detector.

            Args:
                log_path (str): The path to the log file.

            Returns:
                logging.Logger: The configured logger object.
        """
        logger = logging.getLogger("MalwareExpert")
        logger.setLevel(logging.INFO)

        formatter = logging.Formatter('%(asctime)s - %(filename)s - %(levelname)s - %(message)s')
        file_handler = logging.FileHandler(log_path, mode='w')
        file_handler.setLevel(logging.INFO)
        file_handler.setFormatter(formatter)

        stream_handler = logging.StreamHandler()
        stream_handler.setFormatter(formatter)

        logger.addHandler(file_handler)
        logger.addHandler(stream_handler)

        logger.info('Program has started')
        return logger

    def extractFeature(self, purpose: str='train') -> Any:
        """
        Extract features from the dataset.

        Args:
            purpose (str): The purpose of feature extraction ('train', 'test', or 'predict').
        """
        self.logger.info('Extracting features from the dataset...')
        if purpose == 'train':
            current_data = self.train_data
            input_path = self.config.folder.dataset
        elif purpose == 'test':
            current_data = self.test_data
            input_path = self.config.folder.dataset
        elif purpose == 'predict':
            current_data = self.predict_data
            input_path = self.config.folder.predict

        os.makedirs(self.config.folder.feature, exist_ok=True)
        print('input path:', input_path)
        print('output path:', self.config.folder.feature)

        for data in current_data:
            # if data already extracted, skip
            if os.path.exists(self.config.folder.feature + data):
                print(f'{data} already extracted')
                continue
            else:
                data = os.path.join(input_path, data)
                bin2asm.cli(data, self.config.folder.feature, 3)
        return None

    def vectorize(self, purpose: str='train') -> np.array:
        """
        Vectorize the extracted features.

        Args:
            purpose (str): The purpose of vectorization ('train', 'test', or 'predict').
        """
        self.logger.info('Vectorizing the features...')
        print('Vectorizing the features...')
        os.makedirs(self.config.folder.vectorize, exist_ok=True)
            
        ipath = self.config.folder.feature #asm files directory
        mpath = self.config.path.asm2vec_model #model path

        if purpose == 'train':
            current_data = self.train_data
            dataset = self.config.folder.dataset #binary files
        elif purpose == 'test':
            current_data = self.test_data
            dataset = self.config.folder.dataset
        elif purpose == 'predict':
            current_data = self.predict_data
            dataset = self.config.folder.predict

        print(f'ipath: {ipath}')
        print(f'mpath: {mpath}')
        print(f'dataset: {dataset}')
        print(f'vectorize: {self.config.folder.vectorize}')

        for folder in current_data:  #asm files directory
            dir = os.path.join(ipath, folder)
            # if dir already vectorized, skip
            if os.path.exists(self.config.folder.vectorize + folder + '.dot'):
                print(f'{dir} already vectorized')
                continue
            else:
                print(f'now vectorize: {dir}')
                for file in current_data: #binary files
                    data = os.path.join(dataset, file)
                    dir_temp = dir.split('/')[-1]
                    data_temp = data.split('/')[-1]
                    if dir_temp == data_temp:
                        r2 = r2pipe.open(data)
                        r2.cmd("aaa")
                        r2.cmd("agCd > output.dot")

                        # netwokx graph from dot string
                        G = nx.nx_pydot.read_dot("output.dot")
                        try:
                            for file in os.listdir(dir):
                                file = os.path.join(dir, file)
                                offset, embedding = test.cli(file, mpath)

                                offset_temp = str(offset)[2:]
                                for i in range(8-len(offset_temp)):
                                    offset_temp = '0' + offset_temp
                                offset_temp = '0x' + offset_temp

                                # find the node in the graph and add the embedding to the node
                                if G.has_node(offset_temp):
                                    G.nodes[offset_temp]['embedding'] = embedding
                        except:
                            # with open("error.txt", "a") as f:
                            #     f.write(dir + '\n')
                            # print(f'error in {dir}')
                            pass

                        #output the graph
                        nx.nx_pydot.write_dot(G, self.config.folder.vectorize+"{}.dot".format(data_temp))
                        break

        # delete the output.dot file
        if os.path.exists("output.dot"):
            os.remove("output.dot")

    def model(self, training: bool=True) -> Any:
        """
        Train or load the model.

        Args:
            training (bool): If True, train the model; if False, load an existing model.

        Returns:
            Optional[MalwareExpertModel]: The trained or loaded model.
        """
        # print(f'current model: model_{self.shard_index}_{self.slice_index}.pt')
        if training:
            if self.train_data.__len__() <= 1:
                raise ValueError('Please provide more than 1 training data')
            # a normal training process, no sisa
            print(f'Training model_{self.shard_index}_{self.slice_index}.pt...')
            self.logger.info(f'Training model_{self.shard_index}_{self.slice_index}.pt...')
            start = time()
            # load data
            feature_matrix, edge_index, label, name = self.malware_analyzer.load_asm2vec_data(embedding_files=self.train_data)
            x = [torch.tensor(feature_matrix[i], dtype=torch.float) for i in range(len(feature_matrix))]
            edge_index = [torch.tensor(edge_index[i], dtype=torch.long) for i in range(len(edge_index))]
            y = [torch.tensor(label[i], dtype=torch.long) for i in range(len(label))]
            name = [name[i] for i in range(len(name))]
            all_data = [torch_geometric.data.Data(x=x[i], edge_index=edge_index[i], y=y[i], name=name[i]) for i in range(len(x))]

            # create model
            if self.model_params == None:
                model = MalwareExpertModel(200, self.config.model.hidden_dim, self.config)
            else:
                model = self.model_params

            # criterion and optimizer
            criterion = torch.nn.CrossEntropyLoss()
            optimizers = torch.optim.Adam(model.parameters(), lr=self.config.model.learning_rate)

            # train 0.9
            train_data = all_data[:int(len(all_data)*0.9)]
            train_data = torch_geometric.data.Batch.from_data_list(train_data)
            train_data = torch_geometric.loader.DataLoader(train_data, batch_size=self.config.model.batch_size, shuffle=True)
            # val 0.1
            val_data = all_data[int(len(all_data)*0.9):]
            val_data = torch_geometric.data.Batch.from_data_list(val_data)
            val_data = torch_geometric.loader.DataLoader(val_data, batch_size=self.config.model.batch_size, shuffle=True)
            
            print('training...')
            self.logger.info('training...')
            for epoch in range(self.config.model.epoch):
                train_loss = self.malware_analyzer.train(model, train_data, criterion, optimizers)
                print(all_data)
                print('time: ', time()-start)
                print('epoch: ', epoch, ' loss: ', train_loss)
                self.train_loss.append(train_loss)
                # validation every epoch
                val_loss = 0
                model.eval()
                with torch.no_grad():
                    for data in val_data:
                        output = model(data.x, data.edge_index, data)
                        pred = output.argmax(dim=1)
                        correct = pred.eq(data.y).sum().item()
                        loss = criterion(output, data.y)
                        val_loss += loss.item()
                    print('val_accuracy: ', correct/len(data.y), 'val_loss: ', val_loss)
                    self.val_loss.append(val_loss)
            
            # save the model
            self.model_params = model

        else:
            # load model for later use
            print(f'Loading model_{self.shard_index}_{self.slice_index}.pt...')
            self.logger.info(f'Loading model_{self.shard_index}_{self.slice_index}.pt...')
            self.model_params = MalwareExpertModel(200, self.config.model.hidden_dim, self.config)
            self.model_params.load_state_dict(torch.load(self.config.folder.model + f'slice_models/model_{self.shard_index}_{self.slice_index}.pt'))
            return self.model_params

    def predict(self, purpose: str='test') -> np.array:
        """
        Make predictions using the trained model.

        Args:
            purpose (str): The purpose of prediction ('test' or 'predict').

        Returns:
            np.array: The predicted labels.
        """
        # normal prediction process, no majority vote
        print(f'Predicting with model_{self.shard_index}_{self.slice_index}.pt...')
        self.logger.info(f'Predicting with model_{self.shard_index}_{self.slice_index}.pt...')
        if purpose == 'test':
            current_data = self.test_data
        elif purpose == 'predict':
            current_data = self.predict_data

        # load data
        feature_matrix, edge_index, name = self.malware_analyzer.load_asm2vec_data(embedding_files=current_data, predict=True)
        x = [torch.tensor(feature_matrix[i], dtype=torch.float) for i in range(len(feature_matrix))]
        edge_index = [torch.tensor(edge_index[i], dtype=torch.long) for i in range(len(edge_index))]
        name = name
        all_data = [torch_geometric.data.Data(x=x[i], edge_index=edge_index[i], name=name[i]) for i in range(len(x))]

        # predict the dataset
        predict = [] # list of predicted labels
        for data in all_data:
            model = self.model_params
            model.eval()
            with torch.no_grad():
                output = model(data.x, data.edge_index, data)
                pred = output.argmax(dim=1)
                predict.append(pred.item())
        
        print('Predicted labels:', predict)
        return predict

    def explain(self) -> Any:
        """
        Explain the model's predictions using GNNexplainer, edge pruning, and node pruning.
        """
        print(f'Explaining with model_{self.shard_index}_{self.slice_index}.pt...')
        self.logger.info(f'Explaining with model_{self.shard_index}_{self.slice_index}.pt...')
        current_data = self.predict_data #(or explain_data)

        # load data
        feature_matrix, edge_index, name = self.malware_analyzer.load_asm2vec_data(embedding_files=current_data, predict=True)
        x = [torch.tensor(feature_matrix[i], dtype=torch.float) for i in range(len(feature_matrix))]
        edge_index = [torch.tensor(edge_index[i], dtype=torch.long) for i in range(len(edge_index))]
        name = name
        all_data = [torch_geometric.data.Data(x=x[i], edge_index=edge_index[i], name=name[i]) for i in range(len(x))]
        
        # explain the model
        model = self.model_params
        self.malware_analyzer.GNNexplain(model, self.shard_index, self.slice_index, all_data)
        self.malware_analyzer.edge_pruning(model, self.shard_index, self.slice_index, all_data)
        self.malware_analyzer.node_pruning(model, self.shard_index, self.slice_index, all_data)

    